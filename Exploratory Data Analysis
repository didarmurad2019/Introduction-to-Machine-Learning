# Step 1: Import Essential Libraries.......................
# Import essential data analysis and visualization libraries
import numpy as np  # Numerical computing for arrays and matrices
import pandas as pd  # Data manipulation and analysis with DataFrames
import matplotlib.pyplot as plt  # Core plotting library for static visualizations
import seaborn as sns  # Statistical data visualization built on matplotlib
import warnings  # Manage warning messages during execution

# Suppress warning messages for cleaner output during analysis
warnings.filterwarnings('ignore')

# Configure visualization aesthetics
sns.set_style("whitegrid")  # Apply Seaborn's whitegrid style for clean plots
plt.rcParams['figure.figsize'] = (10, 6)  # Set default figure size to 10x6 inches

# Step 2: Load and First Look at Data.................................

# Load dataset from CSV file (example uses Titanic dataset, replace with your actual file)
# Note: Update 'your_dataset.csv' with the actual path to your dataset file
df = pd.read_csv('your_dataset.csv')

# Quick overview of dataset dimensions (rows, columns)
print("Dataset Shape:", df.shape)  # Returns tuple (number_of_rows, number_of_columns)

# Display first 5 rows to preview data structure and values
print("\nFirst 5 rows:")
print(df.head())  # Shows column names and initial data entries

# Print dataset metadata: column names, data types, non-null counts, and memory usage
print("\nDataset Info:")
print(df.info())  # Essential for understanding data types and missing value patterns

# Generate descriptive statistics for numerical columns
print("\nBasic Statistics:")
print(df.describe())  # Includes count, mean, std, min, percentiles, max for numeric data

# Check for missing values in each column to identify data quality issues
print("\nMissing Values:")
print(df.isnull().sum())  # Count of null/NaN values per column, crucial for data cleaning

# Step 3: Check Data Types and Structure.......................................

# Display data types for each column to understand the structure of the dataset
# This helps identify categorical vs numerical variables for appropriate analysis
print("Data Types:")
print(df.dtypes)  # Shows pandas dtype (int64, float64, object, bool, datetime64, etc.)

# Analyze unique values in each column to assess feature cardinality
# High cardinality may indicate ID-like features, while low cardinality suggests categorical variables
print("\nUnique values per column:")
for col in df.columns:
    # Count distinct values for each column; useful for feature engineering decisions
    print(f"{col}: {df[col].nunique()} unique values")  
    # Note: nunique() excludes NaN values by default (use dropna=False to include them)

# Step 4: Handle Missing Values.................................................

# Visualize missing values using a heatmap for pattern identification
# This helps identify if missing values are random or follow a specific pattern
plt.figure(figsize=(12, 6))
sns.heatmap(df.isnull(),  # Create boolean matrix where True indicates missing value
            yticklabels=False,  # Hide y-axis tick labels for cleaner visualization
            cbar=False,  # Remove color bar since we only show missing/not missing
            cmap='viridis')  # Color scheme: yellow=missing, purple=non-missing
plt.title('Missing Values Heatmap')
plt.show()

# Calculate and display missing value percentages for each column
# Understanding the proportion helps decide appropriate handling strategies
missing_percent = (df.isnull().sum() / len(df)) * 100  # Calculate percentage missing per column

# Create a DataFrame to organize missing value information
missing_df = pd.DataFrame({
    'column': df.columns,
    'missing_percent': missing_percent.values  # Store calculated percentages
})

# Filter to show only columns with missing values, sorted from highest to lowest
missing_df = missing_df[missing_df['missing_percent'] > 0].sort_values('missing_percent', ascending=False)

print("Columns with missing values:")
print(missing_df)

# Strategy: Impute missing values based on column data type
# Different approaches for categorical vs numerical variables
for col in df.columns:
    if df[col].isnull().sum() > 0:  # Only process columns with missing values
        if df[col].dtype == 'object':  # Check if column is categorical (object dtype)
            # Fill categorical missing values with the mode (most frequent value)
            df[col].fillna(df[col].mode()[0], inplace=True)  # [0] gets first mode if multiple
        else:
            # Fill numerical missing values with the median (robust to outliers)
            df[col].fillna(df[col].median(), inplace=True)

# Note: Alternative strategies could include:
# - Dropping columns with >50% missing values
# - Using more advanced imputation (KNN, MICE)
# - Creating indicator variables for missingness
# - Domain-specific imputation

# Step 5: Univariate Analysis......................................
# Numerical Features

# Select numerical columns for analysis
# np.number includes int and float dtypes, excluding categorical/object and datetime
num_cols = df.select_dtypes(include=[np.number]).columns

# Create distribution visualization for each numerical feature
# Subplot grid: rows = number of numerical columns, columns = 2 (histogram + boxplot)
fig, axes = plt.subplots(len(num_cols), 2, figsize=(15, 5*len(num_cols)))

# Iterate through each numerical column and create visualizations
for i, col in enumerate(num_cols):
    
    # Left column: Histogram with Kernel Density Estimate (KDE)
    # Histogram shows frequency distribution, KDE shows smoothed probability density
    sns.histplot(df[col], kde=True, ax=axes[i, 0])
    axes[i, 0].set_title(f'Distribution of {col}')
    axes[i, 0].set_xlabel(col)  # Set x-axis label
    axes[i, 0].set_ylabel('Frequency')  # Set y-axis label
    
    # Right column: Box plot for outlier detection and quartile visualization
    # Shows median, quartiles, and potential outliers
    sns.boxplot(y=df[col], ax=axes[i, 1])
    axes[i, 1].set_title(f'Box Plot of {col}')
    axes[i, 1].set_ylabel(col)  # Set y-axis label

# Adjust subplot spacing to prevent label overlapping
plt.tight_layout()
plt.show()

# Print comprehensive statistical summary for each numerical column
print("Numerical Features Summary:")
for col in num_cols:
    print(f"\n{col}:")
    print(f"  Mean: {df[col].mean():.2f}")           # Average value
    print(f"  Median: {df[col].median():.2f}")       # Middle value (50th percentile)
    print(f"  Std Dev: {df[col].std():.2f}")         # Measure of spread/variability
    print(f"  Skewness: {df[col].skew():.2f}")       # Measure of asymmetry in distribution
    # Positive skew: right-tailed, Negative skew: left-tailed, 0: symmetric
    print(f"  Kurtosis: {df[col].kurtosis():.2f}")   # Measure of tail heaviness
    # High kurtosis: heavy tails (more outliers), Low kurtosis: light tails

# Additional considerations:
# - For skewed features, consider transformation (log, sqrt, Box-Cox)
# - For high kurtosis, investigate potential outliers
# - Compare mean vs median to assess symmetry (similar values suggest normal distribution)

# Categorical Features

# Select categorical columns for analysis
# 'object' dtype typically includes strings and mixed types; can also include 'category' dtype
cat_cols = df.select_dtypes(include=['object']).columns

# Create bar plots to visualize distribution of categorical features
# Single column layout: each categorical feature gets its own subplot
fig, axes = plt.subplots(len(cat_cols), 1, figsize=(15, 5*len(cat_cols)))

# Handle case when there's only one categorical column
# subplots() returns axes differently for single vs multiple plots
if len(cat_cols) == 1:
    axes = [axes]  # Convert to list for consistent indexing

# Iterate through each categorical column and create visualization
for i, col in enumerate(cat_cols):
    
    # Calculate frequency of each category
    value_counts = df[col].value_counts()
    
    # Create bar chart showing category frequencies
    bars = axes[i].bar(value_counts.index, value_counts.values)
    axes[i].set_title(f'Distribution of {col}')
    axes[i].set_xlabel(col)  # Set x-axis label as column name
    axes[i].set_ylabel('Count')  # Set y-axis label
    
    # Rotate x-axis labels for better readability (especially for long category names)
    axes[i].tick_params(axis='x', rotation=45)
    
    # Add count labels on top of each bar for precise value reading
    for bar, count in zip(bars, value_counts.values):
        height = bar.get_height()  # Get bar height (count value)
        axes[i].text(bar.get_x() + bar.get_width()/2.,  # Center text horizontally on bar
                    height + 0.1,  # Position text slightly above bar
                    f'{count}',  # Display count value
                    ha='center',  # Horizontal alignment: center
                    va='bottom',  # Vertical alignment: bottom of text at position
                    fontsize=9)  # Set font size for readability

# Adjust layout to prevent overlapping elements
plt.tight_layout()
plt.show()

# Additional insights that could be added:
# - Percentage distribution alongside counts
# - Option to limit to top N categories for high-cardinality features
# - Horizontal bar charts for features with many categories
# - Color-coding based on category importance or hierarchy

# Step 6: Bivariate/Multivariate Analysis.....................................

# Calculate and visualize correlation matrix for numerical features
# Correlation measures linear relationships between variables (-1 to 1)
plt.figure(figsize=(12, 8))

# Compute correlation matrix using Pearson correlation coefficient
correlation_matrix = df[num_cols].corr()  # Returns square matrix of correlation values

# Create heatmap visualization of correlation matrix
sns.heatmap(correlation_matrix, 
            annot=True,  # Display correlation values in each cell
            cmap='coolwarm',  # Color scheme: blue=negative, red=positive correlation
            center=0,  # Center color map at 0 (neutral correlation)
            fmt='.2f',  # Format annotation to 2 decimal places
            square=True,  # Make cells square-shaped
            linewidths=0.5)  # Add lines between cells for clarity

plt.title('Correlation Matrix of Numerical Features')
plt.tight_layout()  # Adjust layout to prevent label cutoff
plt.show()

# Identify and report highly correlated feature pairs
# High correlation (|r| > 0.5) may indicate redundancy or multicollinearity
high_corr_pairs = []

# Iterate through upper triangle of correlation matrix (excluding diagonal)
for i in range(len(correlation_matrix.columns)):
    for j in range(i+1, len(correlation_matrix.columns)):
        corr_value = correlation_matrix.iloc[i, j]
        # Check if absolute correlation exceeds threshold
        if abs(corr_value) > 0.5:  # Adjust threshold as needed (0.7 for stricter filtering)
            high_corr_pairs.append((correlation_matrix.columns[i], 
                                  correlation_matrix.columns[j], 
                                  corr_value))

# Sort pairs by absolute correlation strength (strongest first)
high_corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)

print("Highly Correlated Features (|corr| > 0.5):")
for pair in high_corr_pairs:
    feature1, feature2, corr_value = pair
    corr_type = "positive" if corr_value > 0 else "negative"
    print(f"{feature1} vs {feature2}: {corr_value:.3f} ({corr_type} correlation)")

# Interpretation notes:
# - Correlation > 0.7: Consider feature removal or dimensionality reduction
# - Positive correlation: Variables increase together
# - Negative correlation: Variables move in opposite directions
# - Near 0: Little to no linear relationship

# Additional analysis options:
# 1. Visualize specific high-correlation relationships:
#    if high_corr_pairs:
#        fig, ax = plt.subplots(1, 1, figsize=(8, 6))
#        feat1, feat2, _ = high_corr_pairs[0]  # Get strongest correlation
#        sns.scatterplot(data=df, x=feat1, y=feat2, ax=ax)
#        ax.set_title(f'{feat1} vs {feat2} (corr: {corr_value:.3f})')
#        plt.show()
#        
# 2. For feature selection:
#    - Remove one of highly correlated pairs (|r| > 0.8)
#    - Use PCA or other dimensionality reduction techniques
#    - Consider domain knowledge for feature importance

# Step 7: Target Variable Analysis (for Supervised ML).......................

# If you have a target variable (for supervised learning problems)
# Set this to your actual target column name (dependent variable)
target_col = 'target_column_name'  # Change this to your target column

# Check if target column exists in the dataframe
if target_col in df.columns:
    print(f"Target Variable: {target_col}")
    print(f"Target dtype: {df[target_col].dtype}")
    print(f"Unique values in target: {df[target_col].nunique()}")
    
    # Create visualizations to understand target variable distribution
    plt.figure(figsize=(12, 5))
    
    # Left plot: Distribution of target variable
    plt.subplot(1, 2, 1)
    if df[target_col].dtype == 'object':
        # For categorical targets (classification problems)
        # Count plot shows frequency of each class - important for class imbalance detection
        sns.countplot(data=df, x=target_col)
        plt.title('Target Distribution (Categorical)')
        plt.xlabel(target_col)
        plt.ylabel('Count')
        
        # Add count labels on bars for clarity
        ax = plt.gca()
        for p in ax.patches:
            ax.annotate(f'{p.get_height():.0f}', 
                       (p.get_x() + p.get_width() / 2., p.get_height()), 
                       ha='center', va='center', xytext=(0, 5), textcoords='offset points')
    else:
        # For numerical targets (regression problems)
        # Histogram with KDE shows distribution shape, skewness, and potential outliers
        sns.histplot(df[target_col], kde=True)
        plt.title('Target Distribution (Numerical)')
        plt.xlabel(target_col)
        plt.ylabel('Frequency')
        
        # Add vertical lines for mean and median
        plt.axvline(df[target_col].mean(), color='red', linestyle='--', label=f'Mean: {df[target_col].mean():.2f}')
        plt.axvline(df[target_col].median(), color='green', linestyle='-', label=f'Median: {df[target_col].median():.2f}')
        plt.legend()
    
    # Right plot: Relationship between target and a key feature
    plt.subplot(1, 2, 2)
    if len(num_cols) > 0:
        # Choose first numerical feature (or select specific feature based on domain knowledge)
        feature = num_cols[0]
        
        if df[target_col].dtype == 'object':
            # For categorical targets: Box plot shows feature distribution per target class
            # Helps identify which features differentiate between classes
            sns.boxplot(data=df, x=target_col, y=feature)
            plt.title(f'{feature} Distribution by {target_col}')
            plt.xlabel(target_col)
            plt.ylabel(feature)
        else:
            # For numerical targets: Scatter plot shows relationship between feature and target
            # Helps identify linear/non-linear relationships and outliers
            sns.scatterplot(data=df, x=feature, y=target_col)
            plt.title(f'{target_col} vs {feature}')
            plt.xlabel(feature)
            plt.ylabel(target_col)
            
            # Add regression line to visualize trend
            sns.regplot(data=df, x=feature, y=target_col, scatter=False, color='red', line_kws={'linewidth': 2})
    else:
        plt.text(0.5, 0.5, 'No numerical features available', 
                horizontalalignment='center', verticalalignment='center')
        plt.title('No Numerical Features')
    
    plt.tight_layout()
    plt.show()
    
    # Additional target analysis that could be added:
    # 1. For classification: Class balance analysis
    #    if df[target_col].dtype == 'object':
    #        class_dist = df[target_col].value_counts(normalize=True) * 100
    #        print(f"\nClass Distribution (%):")
    #        for cls, perc in class_dist.items():
    #            print(f"  {cls}: {perc:.1f}%")
    #    
    # 2. For regression: Basic statistics
    #    else:
    #        print(f"\nTarget Statistics:")
    #        print(f"  Min: {df[target_col].min():.2f}")
    #        print(f"  Max: {df[target_col].max():.2f}")
    #        print(f"  Range: {df[target_col].max() - df[target_col].min():.2f}")
    #        print(f"  Skewness: {df[target_col].skew():.3f}")
    #        print(f"  Kurtosis: {df[target_col].kurtosis():.3f}")
    
else:
    print(f"Warning: '{target_col}' not found in dataframe columns.")
    print(f"Available columns: {list(df.columns)}")
    print("\nFor unsupervised learning or if you don't have a target variable,")
    print("proceed with exploratory analysis without target-specific visualizations.")

# Step 8: Outlier Detection.........................

# Outlier Detection using Interquartile Range (IQR) Method
# IQR is robust to extreme values and works well for most distributions
def detect_outliers_iqr(data, column):
    """
    Detect outliers using IQR method for a specific column.
    
    Parameters:
    -----------
    data : pandas DataFrame
        The dataset containing the column
    column : str
        The column name to analyze for outliers
    
    Returns:
    --------
    pandas DataFrame
        Rows containing outliers in the specified column
    
    Methodology:
    ------------
    1. Calculate Q1 (25th percentile) and Q3 (75th percentile)
    2. Compute IQR = Q3 - Q1
    3. Define bounds: Q1 - 1.5*IQR (lower) and Q3 + 1.5*IQR (upper)
    4. Values outside these bounds are considered outliers
    """
    # Calculate quartiles
    Q1 = data[column].quantile(0.25)  # First quartile (25th percentile)
    Q3 = data[column].quantile(0.75)  # Third quartile (75th percentile)
    IQR = Q3 - Q1  # Interquartile Range (middle 50% of data)
    
    # Define outlier boundaries (1.5*IQR is standard, can adjust for sensitivity)
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    # Identify rows with values outside the boundaries
    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]
    
    return outliers, lower_bound, upper_bound, Q1, Q3, IQR

# Analyze outliers for each numerical column
print("Outlier Analysis using IQR Method:")
print("=" * 50)

for col in num_cols:
    # Detect outliers for current column
    outliers, lower_bound, upper_bound, Q1, Q3, IQR = detect_outliers_iqr(df, col)
    
    if len(outliers) > 0:
        # Calculate percentage of outliers
        outlier_percentage = len(outliers) / len(df) * 100
        
        print(f"\n{col}:")
        print(f"  IQR Range: [{Q1:.2f}, {Q3:.2f}] (IQR: {IQR:.2f})")
        print(f"  Bounds: [{lower_bound:.2f}, {upper_bound:.2f}]")
        print(f"  Outliers: {len(outliers)} rows ({outlier_percentage:.2f}%)")
        
        # Show summary statistics of outliers
        print(f"  Outlier min: {outliers[col].min():.2f}")
        print(f"  Outlier max: {outliers[col].max():.2f}")
        
        # Optional: Show a few example outlier values
        if len(outliers) <= 5:  # If few outliers, show all
            print(f"  Outlier values: {sorted(outliers[col].unique())}")
    else:
        print(f"\n{col}: No outliers detected (0%)")

print("\n" + "=" * 50)
print("Interpretation & Next Steps:")
print("- Consider business context before treating outliers")
print("- For high outlier percentages (>5%), investigate data quality")
print("- For critical features, visualize outliers before deciding treatment")
print("- Options: Keep, Remove, Transform (log, sqrt), or Cap/Winsorize")

# Optional: Visualize outliers for the first few numerical columns
if len(num_cols) > 0:
    print("\nVisualizing outliers for first 3 numerical features...")
    fig, axes = plt.subplots(min(3, len(num_cols)), 2, figsize=(15, 4 * min(3, len(num_cols))))
    
    if min(3, len(num_cols)) == 1:
        axes = [axes]  # Handle single subplot case
    
    for i, col in enumerate(num_cols[:3]):
        # Left: Box plot highlighting outliers
        sns.boxplot(y=df[col], ax=axes[i, 0], color='skyblue')
        axes[i, 0].set_title(f'Box Plot: {col}')
        axes[i, 0].set_ylabel(col)
        
        # Right: Scatter plot showing outlier positions
        x_positions = np.random.normal(0, 0.1, size=len(df[col]))  # Jitter for better visualization
        axes[i, 1].scatter(x_positions, df[col], alpha=0.5)
        
        # Mark outliers with different color
        outliers, _, upper_bound, _, _, _ = detect_outliers_iqr(df, col)
        if len(outliers) > 0:
            outlier_indices = outliers.index
            outlier_positions = x_positions[outlier_indices]
            axes[i, 1].scatter(outlier_positions, outliers[col], color='red', label='Outliers', alpha=0.7)
            
        axes[i, 1].axhline(y=upper_bound, color='orange', linestyle='--', label='Upper Bound')
        axes[i, 1].set_title(f'Outlier Scatter: {col}')
        axes[i, 1].set_ylabel(col)
        axes[i, 1].legend()
        axes[i, 1].set_xlim(-0.5, 0.5)
    
    plt.tight_layout()
    plt.show()

# Step 9: Correlation Analysis

# ========================================================
# CORRELATION ANALYSIS
# ========================================================
# Correlation analysis helps understand linear relationships between numerical features
# and the target variable (survival in this case)
print("=== CORRELATION ANALYSIS ===")
print("=" * 50)

# First, check which numerical columns we have
# This ensures we only analyze columns that exist in the dataframe
print("Available numerical columns:")
available_num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
print(available_num_cols)
print(f"Total: {len(available_num_cols)} numerical columns")

# Select numerical features for correlation analysis
# We're interested in specific predefined features; adjust this list as needed
num_features = []
target_column = 'survived'  # Define target variable for correlation analysis

# List of features to analyze (customize based on your dataset)
feature_list = ['survived', 'pclass', 'age', 'sibsp', 'parch', 'fare', 'has_deck']

print(f"\nChecking for predefined features:")
for col in feature_list:
    if col in df.columns:
        num_features.append(col)
        print(f"  âœ“ {col} found")
    else:
        print(f"  âœ— Warning: '{col}' not found in dataframe")

print(f"\nUsing features for correlation analysis: {num_features}")

# Calculate correlation matrix using Pearson correlation coefficient
# Values range from -1 (perfect negative correlation) to +1 (perfect positive correlation)
corr_matrix = df[num_features].corr(method='pearson')  # Default method

print(f"\nCorrelation Matrix (Pearson correlation coefficients):")
print(corr_matrix.round(3))  # Round to 3 decimal places for readability

# Visualize correlation matrix using heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, 
            annot=True,        # Display correlation values in each cell
            cmap='coolwarm',   # Blue for negative, red for positive correlation
            center=0,          # Center colormap at zero correlation
            square=True,       # Make cells square
            linewidths=1,      # Add grid lines between cells
            linecolor='white', # White grid lines
            cbar_kws={"shrink": 0.8},  # Adjust color bar size
            fmt='.2f',         # Format annotation to 2 decimal places
            annot_kws={'size': 10},  # Annotation font size
            vmin=-1, vmax=1)   # Set color scale limits

plt.title('Correlation Matrix of Numerical Features', fontsize=14, fontweight='bold')
plt.tight_layout()
plt.show()

# ========================================================
# INTERPRETING CORRELATIONS WITH SURVIVAL
# ========================================================
print("\n" + "=" * 50)
print("ðŸ“ˆ TOP FEATURES CORRELATED WITH SURVIVAL")
print("=" * 50)

# Extract correlations with the target variable
survival_corr = corr_matrix[target_column].sort_values(ascending=False)

print(f"\nAll correlations with '{target_column}' (sorted by strength):")
for feature, corr in survival_corr.items():
    if feature != target_column:
        strength = "Strong" if abs(corr) > 0.5 else "Moderate" if abs(corr) > 0.3 else "Weak"
        direction = "Positive" if corr > 0 else "Negative"
        print(f"  {feature:10} | {corr:+.3f} | {direction:8} | {strength:7} correlation")

# Categorize correlations by direction and strength
print("\n" + "=" * 50)
print("Positive correlation (higher value = more likely to survive):")
print("-" * 30)
for feature, corr in survival_corr.items():
    if feature != target_column and corr > 0:
        print(f"  âž• {feature:10}: {corr:.3f} {'â–ˆ' * int(abs(corr) * 20)}")

print("\nNegative correlation (higher value = less likely to survive):")
print("-" * 30)
for feature, corr in survival_corr.items():
    if feature != target_column and corr < 0:
        print(f"  âž– {feature:10}: {corr:.3f} {'â–ˆ' * int(abs(corr) * 20)}")

print("\nNear-zero correlation (little linear relationship with survival):")
print("-" * 30)
for feature, corr in survival_corr.items():
    if feature != target_column and abs(corr) < 0.1:
        print(f"  â†”ï¸  {feature:10}: {corr:.3f}")

print("\n" + "=" * 50)
print("INTERPRETATION GUIDE:")
print("-" * 30)
print("â€¢ Strong correlation: |r| > 0.5")
print("â€¢ Moderate correlation: 0.3 < |r| < 0.5")
print("â€¢ Weak correlation: 0.1 < |r| < 0.3")
print("â€¢ Negligible correlation: |r| < 0.1")
print("\nNote: Correlation â‰  Causation")
print("High correlation suggests relationship but doesn't prove cause-effect")

# ========================================================
# ADDITIONAL INSIGHTS
# ========================================================
print("\n" + "=" * 50)
print("ðŸ” ADDITIONAL INSIGHTS")
print("=" * 50)

# Feature pairs with high correlation (potential multicollinearity)
print("\nHigh feature-feature correlations (|r| > 0.7) to check for multicollinearity:")
high_corr_pairs = []
for i in range(len(corr_matrix.columns)):
    for j in range(i+1, len(corr_matrix.columns)):
        corr_val = corr_matrix.iloc[i, j]
        if abs(corr_val) > 0.7 and corr_matrix.columns[i] != target_column and corr_matrix.columns[j] != target_column:
            high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_val))

if high_corr_pairs:
    for feat1, feat2, corr_val in high_corr_pairs:
        print(f"  âš  {feat1} â†” {feat2}: {corr_val:.3f}")
    print("  Note: High correlation between features may cause multicollinearity in regression models")
else:
    print("  No concerning multicollinearity detected (|r| < 0.7 for all feature pairs)")

# Statistical significance of correlations (optional)
print(f"\nNote: For n={len(df)} samples, correlation is statistically significant at p<0.05 if |r| > {1.96/np.sqrt(len(df)):.3f}")


# Step 10: Feature Engineering Insights.....................................

# Check for constant or quasi-constant features
# These features provide little to no predictive power and can be removed
# Constant features: Same value for all observations
# Quasi-constant features: Very low variance (e.g., >99% same value)

constant_features = []  # Store features with only one unique value
quasi_constant_features = []  # Store features with very low variance

print("Constant and Quasi-Constant Feature Analysis:")
print("=" * 50)

# Define threshold for quasi-constant features (adjust as needed)
quasi_constant_threshold = 0.01  # 1% - features with <1% unique values are flagged

for col in df.columns:
    # Calculate number of unique values in the column
    unique_count = df[col].nunique(dropna=True)  # Exclude NaN when counting uniques
    unique_ratio = unique_count / len(df)  # Proportion of unique values
    
    # Check for constant features (only one unique value)
    if unique_count == 1:
        constant_features.append(col)
        print(f"âœ“ Constant feature detected: '{col}'")
        print(f"  - Single value: '{df[col].iloc[0]}'")
        print(f"  - Feature provides zero variance")
    
    # Check for quasi-constant features (very low variance)
    elif unique_ratio < quasi_constant_threshold:
        quasi_constant_features.append((col, unique_count, unique_ratio * 100))
        print(f"âš  Quasi-constant feature detected: '{col}'")
        print(f"  - Unique values: {unique_count}")
        print(f"  - Percentage of unique values: {unique_ratio * 100:.2f}%")
        print(f"  - Value distribution:")
        
        # Show value counts for the top few values
        value_counts = df[col].value_counts(dropna=True).head(5)
        for value, count in value_counts.items():
            percentage = count / len(df) * 100
            print(f"    '{value}': {count} rows ({percentage:.1f}%)")
        print()

print("=" * 50)

# Summary of findings
if constant_features:
    print(f"\nConstant Features ({len(constant_features)} found):")
    for i, col in enumerate(constant_features, 1):
        print(f"  {i}. {col}")
else:
    print("\nNo constant features found.")

if quasi_constant_features:
    print(f"\nQuasi-Constant Features ({len(quasi_constant_features)} found, <{quasi_constant_threshold*100:.0f}% unique values):")
    quasi_constant_features.sort(key=lambda x: x[2])  # Sort by uniqueness ratio (ascending)
    for i, (col, unique_count, unique_percent) in enumerate(quasi_constant_features, 1):
        print(f"  {i}. {col}: {unique_count} unique values ({unique_percent:.2f}%)")
else:
    print(f"\nNo quasi-constant features found (threshold: <{quasi_constant_threshold*100:.0f}% unique values).")

print("\n" + "=" * 50)
print("Recommended Actions:")
print("1. Constant features: Remove (df.drop(columns=constant_features, inplace=True))")
print("2. Quasi-constant features:")
print("   - Evaluate business importance")
print("   - Consider removing if not domain-critical")
print("   - Could be useful as indicators (e.g., 'rare event' flags)")
print("3. Consider feature variance threshold for removal (e.g., VarianceThreshold in sklearn)")

# Optional: Statistical test for feature variance
print("\n" + "=" * 50)
print("Statistical Variance Analysis (numerical features only):")
print("-" * 30)

# Calculate variance for numerical features
for col in num_cols:
    variance = df[col].var()
    std_dev = df[col].std()
    cv = std_dev / df[col].mean() if df[col].mean() != 0 else np.inf  # Coefficient of Variation
    
    print(f"{col}:")
    print(f"  Variance: {variance:.4f}")
    print(f"  Std Dev: {std_dev:.4f}")
    print(f"  Coeff of Variation: {cv:.4f}")
    
    # Flag low variance features
    if variance < 0.01:  # Threshold for low variance
        print(f"  âš  Low variance warning (consider removal)")
    print

# Step 10: Export Cleaned Data and Report......................................



